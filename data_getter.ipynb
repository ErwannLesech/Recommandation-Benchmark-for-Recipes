{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a2bdc08",
   "metadata": {},
   "source": [
    "# Food Recommendation System Project\n",
    "\n",
    "This project aims to build a recommender system using the Food.com Recipes and Reviews dataset. The dataset contains user reviews. \n",
    "This file aims :\n",
    "- To get the dataset from Kaggle: [Food.com - Recipes and Reviews](https://www.kaggle.com/datasets/irkaal/foodcom-recipes-and-reviews) and save it locally\n",
    "- Generate a folder \"dataset\" and subfiles with the whole dataset, train and test datasets after splitting them using `Cornac`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d36e49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/r-one/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/r-one/.cache/kagglehub/datasets/irkaal/foodcom-recipes-and-reviews/versions/2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "path = kagglehub.dataset_download(\"irkaal/foodcom-recipes-and-reviews\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd6d2e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in dataset: ['reviews.csv', 'recipes.parquet', 'reviews.parquet', 'recipes.csv']\n"
     ]
    }
   ],
   "source": [
    "dataset_path = path\n",
    "print('Files in dataset:', os.listdir(dataset_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acff9738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in dataset: ['reviews.csv', 'recipes.parquet', 'reviews.parquet', 'recipes.csv']\n",
      "Files in data folder: ['reviews.csv', 'clean_reviews.csv', 'recipes.parquet', 'reviews.parquet', 'recipes.csv']\n"
     ]
    }
   ],
   "source": [
    "data_folder = os.path.join(os.getcwd(), \"data\")\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "# Copy all files from dataset_path to \"data\" folder\n",
    "for filename in os.listdir(dataset_path):\n",
    "    src = os.path.join(dataset_path, filename)\n",
    "    dst = os.path.join(data_folder, filename)\n",
    "    if os.path.isfile(src):\n",
    "        shutil.copy2(src, dst)\n",
    "\n",
    "print('Files in dataset:', os.listdir(dataset_path))\n",
    "print('Files in data folder:', os.listdir(data_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "253c492c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes file path: /home/r-one/Documents/epita/recommender_system/Recommandation-Benchmark-for-Recipes/data/recipes.csv\n",
      "Reviews file path: /home/r-one/Documents/epita/recommender_system/Recommandation-Benchmark-for-Recipes/data/reviews.csv\n"
     ]
    }
   ],
   "source": [
    "recipes_path = os.path.join(data_folder, \"recipes.csv\")\n",
    "reviews_path = os.path.join(data_folder, \"reviews.csv\")\n",
    "\n",
    "print(\"Recipes file path:\", recipes_path)\n",
    "print(\"Reviews file path:\", reviews_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa95b6e",
   "metadata": {},
   "source": [
    "## Clean dataset\n",
    "\n",
    "As seen in the `data_analysis` file, some fields are not useful for our recommandation task and is limitating our application.\n",
    "\n",
    "This is why we are going to clean our datasets to generate `clean_recipes.csv` (resp. `clean_reviews.csv`) to be handled by Cornac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d56de3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewId</th>\n",
       "      <th>RecipeId</th>\n",
       "      <th>AuthorId</th>\n",
       "      <th>AuthorName</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>DateSubmitted</th>\n",
       "      <th>DateModified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>992</td>\n",
       "      <td>2008</td>\n",
       "      <td>gayg msft</td>\n",
       "      <td>5</td>\n",
       "      <td>better than any you can get at a restaurant!</td>\n",
       "      <td>2000-01-25T21:44:00Z</td>\n",
       "      <td>2000-01-25T21:44:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>4384</td>\n",
       "      <td>1634</td>\n",
       "      <td>Bill Hilbrich</td>\n",
       "      <td>4</td>\n",
       "      <td>I cut back on the mayo, and made up the differ...</td>\n",
       "      <td>2001-10-17T16:49:59Z</td>\n",
       "      <td>2001-10-17T16:49:59Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>4523</td>\n",
       "      <td>2046</td>\n",
       "      <td>Gay Gilmore ckpt</td>\n",
       "      <td>2</td>\n",
       "      <td>i think i did something wrong because i could ...</td>\n",
       "      <td>2000-02-25T09:00:00Z</td>\n",
       "      <td>2000-02-25T09:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>7435</td>\n",
       "      <td>1773</td>\n",
       "      <td>Malarkey Test</td>\n",
       "      <td>5</td>\n",
       "      <td>easily the best i have ever had.  juicy flavor...</td>\n",
       "      <td>2000-03-13T21:15:00Z</td>\n",
       "      <td>2000-03-13T21:15:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>44</td>\n",
       "      <td>2085</td>\n",
       "      <td>Tony Small</td>\n",
       "      <td>5</td>\n",
       "      <td>An excellent dish.</td>\n",
       "      <td>2000-03-28T12:51:00Z</td>\n",
       "      <td>2000-03-28T12:51:00Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ReviewId  RecipeId  AuthorId        AuthorName  Rating  \\\n",
       "0         2       992      2008         gayg msft       5   \n",
       "1         7      4384      1634     Bill Hilbrich       4   \n",
       "2         9      4523      2046  Gay Gilmore ckpt       2   \n",
       "3        13      7435      1773     Malarkey Test       5   \n",
       "4        14        44      2085        Tony Small       5   \n",
       "\n",
       "                                              Review         DateSubmitted  \\\n",
       "0       better than any you can get at a restaurant!  2000-01-25T21:44:00Z   \n",
       "1  I cut back on the mayo, and made up the differ...  2001-10-17T16:49:59Z   \n",
       "2  i think i did something wrong because i could ...  2000-02-25T09:00:00Z   \n",
       "3  easily the best i have ever had.  juicy flavor...  2000-03-13T21:15:00Z   \n",
       "4                                 An excellent dish.  2000-03-28T12:51:00Z   \n",
       "\n",
       "           DateModified  \n",
       "0  2000-01-25T21:44:00Z  \n",
       "1  2001-10-17T16:49:59Z  \n",
       "2  2000-02-25T09:00:00Z  \n",
       "3  2000-03-13T21:15:00Z  \n",
       "4  2000-03-28T12:51:00Z  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv(reviews_path)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4cd906",
   "metadata": {},
   "source": [
    "We check there are no rows with NaN in Rating field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42d822cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews before dropping NaN: 1401982\n",
      "Number of reviews after dropping NaN: 1401982\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of reviews before dropping NaN:\", len(reviews))\n",
    "reviews = reviews.dropna(subset=['Rating'])\n",
    "print(\"Number of reviews after dropping NaN:\", len(reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1db827",
   "metadata": {},
   "source": [
    "We drop rows with missing AuthorId, RecipeId, or Rating as it cancel Reader from Cornac to work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db955c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews before dropping missing values: 1401982\n",
      "Number of reviews after dropping missing values: 1401982\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of reviews before dropping missing values:\", len(reviews))\n",
    "reviews = reviews.dropna(subset=['AuthorId', 'RecipeId', 'Rating'])\n",
    "print(\"Number of reviews after dropping missing values:\", len(reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52482d9",
   "metadata": {},
   "source": [
    "We drop useless columns as `DateSubmitted`, `DateModified`, `Review` and `AuthorName`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89a97c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in reviews: 8\n",
      "Number of columns in reviews after dropping some: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of columns in reviews:\", len(reviews.columns))\n",
    "reviews = reviews.drop(columns=['AuthorName', 'DateSubmitted', 'DateModified', 'Review'])\n",
    "print(\"Number of columns in reviews after dropping some:\", len(reviews.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b3d20e",
   "metadata": {},
   "source": [
    "We save this new clean reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38326732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean reviews saved to /home/r-one/Documents/epita/recommender_system/Recommandation-Benchmark-for-Recipes/data/clean_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "clean_reviews_path = os.path.join(data_folder, \"clean_reviews.csv\")\n",
    "reviews.to_csv(clean_reviews_path, index=False)\n",
    "print(f\"Clean reviews saved to {clean_reviews_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080d6d39",
   "metadata": {},
   "source": [
    "## Test and split data with `Cornac`\n",
    "\n",
    "For this we are going to use:\n",
    "- Reader\n",
    "- Dataset\n",
    "- StratifiedSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b10d6874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cornac\n",
    "from cornac.data import Reader, Dataset\n",
    "from cornac.eval_methods import StratifiedSplit, RatioSplit\n",
    "\n",
    "reader = Reader()\n",
    "data = reader.read(\n",
    "    clean_reviews_path,\n",
    "    sep=',',\n",
    "    skip_lines=1,\n",
    "    user_col='AuthorId',\n",
    "    item_col='RecipeId',\n",
    "    rating_col='Rating'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fe0c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_uir(data, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a831d272",
   "metadata": {},
   "source": [
    "Je n'arrive pas à faire fonctionner StratifiedSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b8dbdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<cornac.eval_methods.ratio_split.RatioSplit object at 0x7f757f006050>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'Dataset' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      4\u001b[39m ratio_split = RatioSplit(\n\u001b[32m      5\u001b[39m     data=data,\n\u001b[32m      6\u001b[39m     test_size=\u001b[32m0.2\u001b[39m,         \n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     seed=\u001b[32m42\u001b[39m                \n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(ratio_split)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNumber of train ratings:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mratio_split\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNumber of test ratings:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(ratio_split.test_set))\n",
      "\u001b[31mTypeError\u001b[39m: object of type 'Dataset' has no len()"
     ]
    }
   ],
   "source": [
    "from cornac.eval_methods import RatioSplit\n",
    "\n",
    "# Create a RatioSplit object for random splitting\n",
    "ratio_split = RatioSplit(\n",
    "    data=data,\n",
    "    test_size=0.2,         \n",
    "    rating_threshold=2.0,\n",
    "    exclude_unknowns=False,\n",
    "    seed=42                \n",
    ")\n",
    "\n",
    "print(ratio_split)\n",
    "print(\"Number of train ratings:\", len(ratio_split.train_set))\n",
    "print(\"Number of test ratings:\", len(ratio_split.test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8478a7e2",
   "metadata": {},
   "source": [
    "Je n'arrive pas à faire fonctionner StratifiedSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a89e5425",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "val_size + test_size (1) should be smaller than data_size=1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m stratified_split = \u001b[43mStratifiedSplit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrating_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexclude_unknowns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/cornac/eval_methods/stratified_split.py:103\u001b[39m, in \u001b[36mStratifiedSplit.__init__\u001b[39m\u001b[34m(self, data, group_by, chrono, fmt, test_size, val_size, rating_threshold, seed, exclude_unknowns, verbose, **kwargs)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28mself\u001b[39m.val_size = val_size\n\u001b[32m    101\u001b[39m \u001b[38;5;28mself\u001b[39m.test_size = test_size\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/cornac/eval_methods/stratified_split.py:123\u001b[39m, in \u001b[36mStratifiedSplit._split\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m rating_indices \u001b[38;5;129;01min\u001b[39;00m grouped_indices.values():\n\u001b[32m    122\u001b[39m     n_ratings = \u001b[38;5;28mlen\u001b[39m(rating_indices)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     n_train, _, n_test = \u001b[43mRatioSplit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_size\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mval_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_ratings\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chrono:\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# training portion is chronologically sorted\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# validation and test portions are randomly selected\u001b[39;00m\n\u001b[32m    130\u001b[39m         rating_indices = (\n\u001b[32m    131\u001b[39m             rating_indices[:n_train]\n\u001b[32m    132\u001b[39m             + \u001b[38;5;28mself\u001b[39m.rng.permutation(rating_indices[n_train:]).tolist()\n\u001b[32m    133\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/cornac/eval_methods/ratio_split.py:108\u001b[39m, in \u001b[36mRatioSplit.validate_size\u001b[39m\u001b[34m(val_size, test_size, data_size)\u001b[39m\n\u001b[32m    106\u001b[39m val_test_size = val_size + test_size\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val_test_size >= data_size:\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    109\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mval_size + test_size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_test_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) should be smaller than data_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    110\u001b[39m     )\n\u001b[32m    112\u001b[39m train_size = data_size - (val_size + test_size)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(train_size), \u001b[38;5;28mint\u001b[39m(val_size), \u001b[38;5;28mint\u001b[39m(test_size)\n",
      "\u001b[31mValueError\u001b[39m: val_size + test_size (1) should be smaller than data_size=1"
     ]
    }
   ],
   "source": [
    "stratified_split = StratifiedSplit(\n",
    "    data=data,\n",
    "    test_size=0.2,\n",
    "    rating_threshold=3.0,\n",
    "    exclude_unknowns=True,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0cc5c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set saved to /home/r-one/Documents/epita/recommender_system/Recommandation-Benchmark-for-Recipes/data/train_reviews.csv (1121586 rows)\n",
      "Test set saved to /home/r-one/Documents/epita/recommender_system/Recommandation-Benchmark-for-Recipes/data/test_reviews.csv (280396 rows)\n",
      "Test set saved to /home/r-one/Documents/epita/recommender_system/Recommandation-Benchmark-for-Recipes/data/test_reviews.csv (280396 rows)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_train_test(df, test_size=0.2, random_state=42, train_path=None, test_path=None):\n",
    "    np.random.seed(random_state)\n",
    "    shuffled_indices = np.random.permutation(len(df))\n",
    "    test_set_size = int(len(df) * test_size)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    train_df = df.iloc[train_indices]\n",
    "    test_df = df.iloc[test_indices]\n",
    "    if train_path is not None:\n",
    "        train_df.to_csv(train_path, index=False)\n",
    "        print(f\"Train set saved to {train_path} ({len(train_df)} rows)\")\n",
    "    if test_path is not None:\n",
    "        test_df.to_csv(test_path, index=False)\n",
    "        print(f\"Test set saved to {test_path} ({len(test_df)} rows)\")\n",
    "    return train_df, test_df\n",
    "\n",
    "train_path = os.path.join(data_folder, \"train_reviews.csv\")\n",
    "test_path = os.path.join(data_folder, \"test_reviews.csv\")\n",
    "train_df, test_df = split_train_test(reviews, test_size=0.2, random_state=42, train_path=train_path, test_path=test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "863b31a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train set: 1121586\n",
      "Size of test set: 280396\n",
      "Size of total dataset: 1401982  == Size of train set + Size of test set: 1401982\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of train set:\", len(train_df))\n",
    "print(\"Size of test set:\", len(test_df))\n",
    "print(\"Size of total dataset:\", len(reviews), \" == Size of train set + Size of test set:\", len(train_df) + len(test_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
